A benchmark for LLMs

Each test will be a conversation where the AI has to interrogate a "black box"
function to find out what the function does.

i.e. each time the LLM will propose a set of inputs to test the function, and
then it will receive the output in the response.

When the LLM is confident it has figured it out, then the system tests the LLM
got it right by asking it what output it would expect the function to provide
from a given input.
